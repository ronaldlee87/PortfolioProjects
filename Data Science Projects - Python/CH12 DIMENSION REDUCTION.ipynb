{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CH12 DIMENSION REDUCTION.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPfIxSgAtWvjpiuEnBN1iPH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"747kLqtJTG3N"},"source":["# CH 12 DIMENSION REDUCTION\n"]},{"cell_type":"markdown","metadata":{"id":"sTNmohJ0taIz"},"source":["# HANDS-ON ANALYSIS"]},{"cell_type":"markdown","metadata":{"id":"bsW8SoVaK4eL"},"source":["---\n","# Use **cereals** Dataset below.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Pxf5VaU2K4WO"},"source":["#21\n","Standardize or normalize the predictors Sugars, Fiber, and Potass.\n"]},{"cell_type":"code","metadata":{"id":"kPKh-JwkuGDx"},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","folder = \"/gdrive/My Drive/Python Practice/Datasets\"\n","\n","import pandas as pd\n","import numpy as np\n","from scipy import stats\n","import statsmodels.api as sm\n","import statsmodels.stats.outliers_influence as inf\n","#PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","cereals = pd.read_csv(folder + '/cereals.CSV')\n","cereals = cereals.dropna()\n","#np.isnan(cereals['Sugars']).sum()\n","#np.isnan(cereals['Fiber']).sum()\n","#np.isnan(cereals['Potass']).sum()\n","\n","cols = ['Sugars', 'Fiber', 'Potass']\n","cereals_z = pd.DataFrame(stats.zscore(cereals[cols]), columns=cols)\n","cereals_z.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-QCSuHT54V5s"},"source":["#22\n","Construct the correlation matrix for Sugars, Fiber, and Potass. <br>\n","Which variables are highly correlated?"]},{"cell_type":"code","metadata":{"id":"vje0wHPw51P0"},"source":["X = pd.DataFrame(cereals[cols], columns = cols)\n","Y = pd.DataFrame(cereals[['Rating']])\n","\n","pd.plotting.scatter_matrix(X, figsize=(10,10))\n","X.corr()\n","#Potass and Fiber are highly correlated"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5r1gLhe454Xl"},"source":["#23\n","Build a regression model to estimate Rating based on Sugars, Fiber, and Potass.<br>\n","Obtain the VIFs from the model. <br>\n","Which VIFs indicate that multicollinearity is a problem?\n"]},{"cell_type":"code","metadata":{"id":"hgine0vT8olc"},"source":["X = sm.add_constant(cereals[cols])\n","Y = pd.DataFrame(cereals[['Rating']])\n","Y.index = X.index\n","\n","m01 = sm.OLS(Y, X).fit()\n","print(m01.summary()) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UIbJb9K19Kql"},"source":["# VIF \n","[inf.variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\n","# Since VIF of Fiber, Potass > 5, it has moderate multicollinearity"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n9JCSu_Z-gj9"},"source":["#24\n","Run PCA using varimax rotation and three components. <br>\n","What percent of the variability is explained by one component? <br>\n","By two components? <br>\n","By all three components?\n"]},{"cell_type":"code","metadata":{"id":"VOE2sg9sNwLn"},"source":["!pip install factor-analyzer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1VAy4wkWMNcE"},"source":["# varimax rotation\n","\n","from factor_analyzer import FactorAnalyzer\n","fa = FactorAnalyzer(n_factors=3, method='principal', rotation=\"varimax\")\n","fa.fit(cereals[cols])\n","\n","#print(fa.loadings_.round(4))       # meaning?    ex) 1st row : [-0.0278  0.9995 -0.0149] means that Component 1 contains Fiber factor (0.9995 > 0)\n","#fa.get_eigenvalues()               # meaning?\n","\n","fa.get_factor_variance()            # 1st row : eigenvalue / 2nd row : variance per each factor / 3rd row : cumulative variance per each factor\n","\n","# One component explains 63.6 % of variability\n","# Two Components explain 97.4% of variability "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"An5Oi1ZN_gvS"},"source":["#no rotation\n","\n","X = pd.DataFrame(cereals[cols], columns = cols)\n","Y = pd.DataFrame(cereals[['Rating']])\n","\n","pca01 = PCA(n_components=3)\n","principComp = pca01.fit_transform(X)    # principComp meaning ?\n","\n","print(pca01.explained_variance_ratio_)              # variability per component 1,2,3\n","print(np.cumsum(pca01.explained_variance_ratio_))   # cumulative variability\n","\n","# Component 1 explains 99 % of variability"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yp5cOzv698Wn"},"source":["#25\n","Make a plot of the eigenvalues of the three components. <br>\n","Using the eigenvalue criterion, how many components would you retain?"]},{"cell_type":"code","metadata":{"id":"B2vditHUJn4E"},"source":["import matplotlib.pyplot as plt\n","\n","cov_mat = np.cov(cereals_z, rowvar=False)\n","evals = np.linalg.eigvals(cov_mat)              # eigen value of covariance matrix... meaning?\n","comp = [1,2,3]\n","\n","plt.plot(comp, evals, color='black')\n","plt.xlabel(\"Component\")\n","plt.ylabel(\"Eigenvalues\")\n","plt.xticks(np.arange(1,4,1))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0sGpp_Kw-jR1"},"source":["#26\n","Say we want to explain at least 70% of the variability. <br>\n","How many components would you retain?\n"]},{"cell_type":"code","metadata":{"id":"wzbmJ8T4KJ1k"},"source":["# eigenvalue from numpy package \n","cov_mat = np.cov(X, rowvar=False)\n","evals = np.linalg.eigvals(cov_mat)  \n","evals\n","# Based on Eigenvalue Criterion (if eigen > 1, would retain that component) , I would retain ONE COMPONENT  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4thwKQYv_9uW"},"source":["# eigenvalue from Factor Analyzer package\n","fa.get_eigenvalues()\n","# Based on Eigenvalue Criterion, TWO COMPONENTS will be retained"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AFMt8cs1-w2k"},"source":["#27\n","Run PCA using varimax rotation and two components. <br>\n","What percent of the variability do the two components explain?"]},{"cell_type":"code","metadata":{"id":"jWHn8DZmB4V2"},"source":["# Using two components, run PCA\n","\n","from factor_analyzer import FactorAnalyzer\n","\n","fa = FactorAnalyzer(n_factors=2, method='principal', rotation=\"varimax\")\n","fa.fit(cereals[cols])\n","\n","fa.get_factor_variance()  \n","# Two components explain 97.4% of variability "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFPYlVAyCg-M"},"source":["#no rotation\n","pca01 = PCA(n_components=2)\n","principComp = pca01.fit_transform(X)\n","\n","pca01.explained_variance_ratio_ \n","# One component explains 99.6% of variability if No rotation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kKUES2cT_JjB"},"source":["#28\n","What variable or variables are contained in Component 1? What variable or variables are contained in Component 2?"]},{"cell_type":"code","metadata":{"id":"V-4FxHWNDPiN"},"source":["fa = FactorAnalyzer(n_factors=3, method='principal', rotation=\"varimax\")\n","fa.fit(cereals[cols])\n","\n","print(fa.loadings_.round(4)) \n","\n","# Components 1 : Fiber (0.9995 > 0)\n","# Components 2 : Sugars (0.9708 > 0), Potass (0.2074 > 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPNWlJjO_T_5"},"source":["#29\n","Use the two components as the predictor variables in a regression model to estimate Rating. <br>\n","What are the regression coefficients of the two components?\n"]},{"cell_type":"code","metadata":{"id":"0Ok4Tuhpbb_J"},"source":["from sklearn.linear_model import LinearRegression\n","\n","X_new = fa.transform(X)\n","X_new = pd.DataFrame(X_new, columns=['P1','P2','P3'])\n","Y_new = pd.DataFrame(cereals['Rating'])\n","\n","X_pca = X_new.iloc[:,:2]\n","\n","reg = LinearRegression().fit(X_pca, Y_new)\n","reg.coef_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vBJngKHxbS3B"},"source":["# Linear Regression (from statsmodel pacakge) \n","import statsmodels.api as sm\n","\n","Y_new.index = X_pca.index\n","\n","reg1 = sm.OLS(Y_new, X_pca).fit()\n","reg1.summary()\n","# coef [p1 : 6.7411, p2: -10.3105]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQ7mbElq_lH1"},"source":["#30\n","What are the VIFs of the two components in the regression model?\n"]},{"cell_type":"code","metadata":{"id":"M-WeL8oY_siY"},"source":["# VIF \n","[inf.variance_inflation_factor(X_pca.values,i) for i in range(X_pca.shape[1])] \n","# VIF is very close to 1. Good\n","# VIF = 1 / (1 - R^2) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IZP919fjL9MU"},"source":["---\n","# Use **red_wine_PCA_training**, **red_wine_PCA_test** Dataset below.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"hIxDlviModTd"},"source":["red_train = pd.read_csv(folder + \"/red_wine_PCA_training\")\n","red_test = pd.read_csv(folder + \"/red_wine_PCA_test\")\n","\n","red_train = red_train.dropna()\n","red_test = red_test.dropna()\n","\n","Y = red_train[['quality']]\n","X_names = ['alcohol', 'residual sugar', 'pH', 'density', 'fixed acidity']\n","X = pd.DataFrame(red_train[X_names])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lv3SRYMBmudi"},"source":["#31\n","Standardize or normalize the predictors."]},{"cell_type":"code","metadata":{"id":"8ZU9AskJoORa"},"source":["red_z = pd.DataFrame(stats.zscore(X), columns=X_names)\n","red_z.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRsjQET7pub2"},"source":["#32\n","Construct the correlation matrix for the predictors. <br>\n","Between which predictors do you find the highest correlations?\n"]},{"cell_type":"code","metadata":{"id":"DS4ZCvTup4Si"},"source":["X.corr()\n","# pH and fixed acidity has the highest correlation (66%)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGcWaaFPp6rK"},"source":["#33\n","Build a regression model to estimate quality based on the predictors. <br>\n","Obtain the VIFs from the model. <br>\n","Which VIFs indicate that multicollinearity is a problem? <br>\n","Compare the variables with high VIF to the correlated variables from the previous exercise.\n"]},{"cell_type":"code","metadata":{"id":"-3FLKSWxqM8S"},"source":["# Linear Regression (from sklearn pacakge) \n","from sklearn.linear_model import LinearRegression\n","\n","X_const = sm.add_constant(X)\n","reg = LinearRegression().fit(X_const, Y)\n","reg.coef_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AIQda57rvjbg"},"source":["# Linear Regression (from statsmodel pacakge) \n","import statsmodels.api as sm\n","\n","X_const = sm.add_constant(X)\n","reg1 = sm.OLS(Y, X_const).fit()\n","reg1.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"55cjeJPYqeIK"},"source":["# VIF \n","X_const = sm.add_constant(X)\n","[inf.variance_inflation_factor(X_const.values,i) for i in range(X_const.shape[1])] \n","# (VIF_density, VIF_fixed acidity) have high VIF\n","# Corr(density, fixed acidity) also has the highest (0.6466)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ThXVysztq_7y"},"source":["#34\n","Perform PCA using varimax rotation. <br>\n","Show the rotated proportions of variance explained for extracting up to five components.<br>\n","What percent of the variability is explained by one component? <br>\n","By two components? <br>\n","By three components? <br>\n","By four components? <br>\n","By all five components?\n"]},{"cell_type":"code","metadata":{"id":"TxCGMzCWyiDy"},"source":["# varimax rotation\n","\n","from factor_analyzer import FactorAnalyzer\n","fa = FactorAnalyzer(n_factors=5, method='principal', rotation=\"varimax\")\n","fa.fit(X)\n","\n","#print(fa.loadings_.round(4))       # meaning?    ex) 1st row : [-0.0278  0.9995 -0.0149] means that Component 1 contains Fiber factor (0.9995 > 0)\n","#fa.get_eigenvalues()               # meaning?\n","\n","fa.get_factor_variance()            # 1st row : eigenvalue / 2nd row : variance per each factor / 3rd row : cumulative variance per each factor\n","\n","# One component explains 29.21 % of variability\n","# Two Components explain 53.79% of variability \n","# Three Components explain 77.01% of variability \n","# Four Components explain 97.87% of variability \n","# All five Components explain 100.00% of variability "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oK6f7U0xz81m"},"source":["#no rotation\n","\n","pca02 = PCA(n_components=5)\n","principComp = pca02.fit_transform(X)\n","\n","print(pca02.explained_variance_ratio_)              # variability per components\n","print(np.cumsum(pca02.explained_variance_ratio_))   # cumulative variability\n","\n","# 3 components explain 99.79 % of variability"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NtfmbWh1ylTC"},"source":["#35\n","Say we want to explain at least 90% of the variability. <br>\n","How many components does the proportion of variance explained criterion suggest we extract?\n"]},{"cell_type":"code","metadata":{"id":"vDtwANZ-ylak"},"source":["# 4 components "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qYGFy3hMyliK"},"source":["#36\n","Make a plot of the eigenvalues of the five components. <br>\n","According to the eigenvalue cri- terion, how many components should we extract?\n"]},{"cell_type":"code","metadata":{"id":"gg5rLvMDylqD"},"source":["import matplotlib.pyplot as plt\n","\n","cov_mat = np.cov(red_z, rowvar=False)\n","evals = np.linalg.eigvals(cov_mat)              # eigen value of covariance matrix... meaning?\n","comp = [1,2,3,4,5]\n","\n","plt.plot(comp, evals, color='black')\n","plt.xlabel(\"Component\")\n","plt.ylabel(\"Eigenvalues\")\n","plt.xticks(np.arange(1,6,1))\n","plt.axhline(y=1.0, color='r',linestyle='dashed')\n","plt.show()\n","\n","# 3 components should be extracted based on the eigenvalue criterion"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V1os4PPoFPg7"},"source":["#37\n","Combine the recommendations from the two criteria to reach a consensus as to how many components we should extract.\n"]},{"cell_type":"code","metadata":{"id":"fIPmIERrEfpn"},"source":["# The Proportion of Variance Explained Criterion : Extract 4 components \n","# The Eigenvalue Criterion : Extract 3 components\n","# Combine these two criterion : Extract 4 components"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2B_sL5yUHfli"},"source":["#38\n","Profile each of your components, stating which variables are included, and noting their within‐component correlation (positive or negative). <br>\n","For simplicity, consider compo- nents weights greater than 0.5 only.\n"]},{"cell_type":"code","metadata":{"id":"fWHuKjNzHw2U"},"source":["# varimax rotation\n","\n","from factor_analyzer import FactorAnalyzer\n","fa = FactorAnalyzer(n_factors=5, method='principal', rotation=\"varimax\")\n","fa.fit(X)\n","\n","print(fa.loadings_.round(4))       # meaning?    ex) 1st row : [-0.0278  0.9995 -0.0149] means that Component 1 contains Fiber factor (0.9995 > 0)\n","#fa.get_factor_variance()            # 1st row : eigenvalue / 2nd row : variance per each factor / 3rd row : cumulative variance per each factor\n","\n","# Principal Component 1 contains pH \n","# Principal Component 2 contains density\n","# Principal Component 3 contains residual sugar\n","# Principal Component 4 contains alcohol\n","# Principal Component 5 contains alcohol"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxJGBBUq1GnR"},"source":["#pca.components_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKXSTxk4HlE6"},"source":["#39\n","Produce the correlation matrix for the components. <br>\n","What do these values mean?\n"]},{"cell_type":"code","metadata":{"id":"y5vm0C9dHwNy"},"source":["pca02 = PCA(n_components=5)\n","X_pca = pca02.fit_transform(X)\n","X_pca = pd.DataFrame(X_pca, columns=X_names)\n","X_pca.corr()\n","\n","# No correlation between predictor variables.\n","# orthogonal to each other"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ezoZ8QSuSHTK"},"source":["#40\n","Next, use only the components you extracted to estimate wine quality using a regression model. <br>\n","Do not include the original predictors.<br>\n","a.  Compare the values of s and R_adj between the PCA regression and the original regres- sion model.<br>\n","b.  Explain why the original model slightly outperformed the PCA model.<br>\n","c.  Explain how the PCA model may be considered superior, even though slightly outperformed?\n"]},{"cell_type":"code","metadata":{"id":"Dr-r-LcOSsOq"},"source":["from sklearn.linear_model import LinearRegression\n","\n","X_new = fa.transform(X)\n","X_new = pd.DataFrame(X_new, columns=['P1','P2','P3', 'P4', 'P5'])\n","Y_new = pd.DataFrame(red_train['quality'])\n","\n","X_pca = X_new.iloc[:,:4]\n","\n","reg = LinearRegression().fit(X_pca, Y_new)\n","reg.coef_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jx5R-L9zUSlI"},"source":["# Linear Regression (from statsmodel pacakge) \n","import statsmodels.api as sm\n","\n","Y_new.index = X_pca.index\n","\n","reg2 = sm.OLS(Y_new, X_pca).fit()\n","\n","print(\"PCA Regression Model\\ns: %.4f \\nR: %.4f\" % (np.sqrt(reg2.scale), reg2.rsquared_adj) )  # s, R_adj \n","print(reg2.summary())  # summary \n","# R: -0.0003\n","# s: 5.6766"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9d1sufJYVt7"},"source":["# Original Regression model \n","reg_orig = sm.OLS(Y, X).fit()\n","\n","print(\"Original Regression Model\\ns: %.4f \\nR: %.4f\" % (np.sqrt(reg_orig.scale), reg_orig.rsquared_adj) )  # s, R_adj \n","print(reg_orig.summary())  # summary\n","# s: 0.6851\n","# R: 0.986"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qvBQE1vb0TY"},"source":["# Why original model outperform ?   s of original model is very smaller than PCA model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PNtd_JBES3Oo"},"source":["#41\n","In your regression from the previous exercise, what are the VIFs of the two components in the regression model? What do these values mean?\n"]},{"cell_type":"code","metadata":{"id":"0dBF12DLcTGi"},"source":["X_pca.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mePwmZZSS5ja"},"source":["# VIF \n","[inf.variance_inflation_factor(X_pca.values,i) for i in range(X_pca.shape[1])] \n","# VIF is very close to 1. Good\n","# VIF = 1 / (1 - R^2) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRPCjZg3tb6x"},"source":[""],"execution_count":null,"outputs":[]}]}